# 생성형 AI를 책임감있게 사용하기

[![Using Generative AI Responsibly](../../images/03-lesson-banner.png?WT.mc_id=academic-105485-koreyst)]() 

> **비디오 출시 예정**

AI, 특히 생성형 AI에 매료되기 쉽지만, 이를 어떻게 책임감 있게 사용할 것인지 고려해야 합니다. 결과물이 공정하고 해롭지 않은지 확인하는 방법 등을 고려해야 합니다. 이 장에서는 고려해야 할 사항, 그리고 AI 사용을 개선하기 위한 적극적인 조치를 취하는 방법을 언급된 맥락과 함께 알려드리고자 합니다.

## 개요

이 강의에서는 다음을 다룹니다:

- 생성형 AI 애플리케이션을 구축할 때 책임감 있는 AI를 우선시해야 하는 이유.
- 책임감 있는 AI의 핵심 원칙과 이것이 생성형 AI와 어떻게 연관되는지.
- 전략과 도구를 통해 이러한 책임감 있는 AI 원칙을 실행에 옮기는 방법.

## 학습 목표

이 강의를 마치면 다음 내용을 알게 될 것입니다: 

- 생성형 AI 애플리케이션을 구축할 때 책임감 있는 AI의 중요성.
- 생성형 AI 애플리케이션을 구축할 때 책임감 있는 AI의 핵심 원칙을 고려하고 적용해야 하는 시기.
- 책임감 있는 AI의 개념을 실제로 적용하기 위해 사용할 수 있는 도구와 전략.

## 책임감 있는 AI 원칙

생성형 AI에 대한 기대가 그 어느 때보다 높습니다. 이러한 열기로 인해 많은 새로운 개발자와 관심, 자금이 이 분야로 유입되고 있습니다. 이는 생성형 AI를 사용하여 제품과 회사를 구축하려는 모든 사람에게 매우 긍정적인 일이지만, 책임감 있게 진행하는 것도 중요합니다.

이 과정을 통해 저희는 스타트업과 AI 교육 제품을 구축하는 데 집중하고 있습니다. 공정성, 포용성, 신뢰성/안전성, 보안 및 개인정보 보호, 투명성 및 책임성이라는 책임감 있는 AI의 원칙을 사용할 것입니다. 이러한 원칙을 통해 이러한 원칙이 제품에서 생성형 AI를 사용하는 것과 어떤 관련이 있는지 살펴볼 것입니다.

## 책임감 있는 AI를 우선시해야 하는 이유

제품을 만들 때 사용자의 최대 관심사를 염두에 두고 인간 중심적인 접근 방식을 취하면 최상의 결과를 얻을 수 있습니다.

생성형 AI는 사용자에게 유용한 답변, 정보, 안내 및 콘텐츠를 생성할 수 있다는 점에서 독보적입니다. 이는 많은 수작업 단계가 없이도 매우 인상적인 결과를 얻을 수 있습니다. 하지만 적절한 계획과 전략이 없다면 안타깝게도 사용자, 제품, 사회 전체에 해로운 결과를 초래할 수도 있습니다.

이러한 잠재적으로 유해한 결과 중 일부(전부는 아님)를 살펴보겠습니다:

### 환각(Hallucinations)

환각은 LLM이 완전히 말도 안 되는 콘텐츠를 생성하거나 다른 정보 출처에 의하면 사실이 아닌 것을 생성하는 현상을 설명하는 데 사용되는 용어입니다.

학생들이 모델에게 역사와 관련된 질문을 할 수 있는 기능을 개발하는 스타트업을 예로 들어 보겠습니다. 한 학생이 '타이타닉호의 유일한 생존자는 누구였나요'라는 질문을 합니다.

이 모델은 아래와 같은 응답을 생성합니다:

![Prompt saying "Who was the sole survivor of the Titanic"](../../../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp?WT.mc_id=academic-105485-koreyst)

> *(Source: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))*

위 내용은 매우 자신감 있고 철저한 답변 태도를 보입니다만, 안타깝게도 이는 잘못된 정보입니다. 최소한의 조사만으로도 타이타닉 호의 생존자가 한 명 이상이라는 것을 알 수 있습니다. 이 주제를 이제 막 연구하기 시작한 학생에게는 이 답변이 의문을 제기하지 않고 사실로 받아들일 만큼 충분히 설득력이 있을 수 있습니다. 그 결과 AI 시스템의 신뢰성이 떨어지고 스타트업의 평판에 부정적인 영향을 미칠 수 있습니다.

어떤 LLM을 사용하든 질문을 반복할 때마다 환각을 최소화하는 데 있어 성능이 개선되는 것을 확인했습니다. 이러한 개선에도 불구하고 애플리케이션 빌더와 사용자는 여전히 이러한 한계를 인식하고 있어야 합니다.

### 유해한 콘텐츠

이전 섹션에서 모델이 부정확하거나 무의미한 응답을 생성하는 경우를 다루었습니다. 모델이 유해한 콘텐츠로 응답할 때 주의해야 할 또 다른 위험도 있습니다.

유해한 콘텐츠는 다음과 같이 정의할 수 있습니다:

- 특정 집단에 자해 또는 위해를 가하도록 지시하거나 조장하는 행위.
- 혐오 또는 비하하는 콘텐츠.
- 모든 유형의 공격 또는 폭력 행위 계획을 안내하는 행위.
- 불법 콘텐츠를 찾거나 불법 행위를 저지르는 방법에 대한 지침을 제공하는 행위.
- 성적으로 노골적인 콘텐츠를 표시하는 행위.

우리 스타트업의 경우, 이러한 유형의 콘텐츠가 학생들에게 노출되지 않도록 적절한 도구와 전략을 마련하고자 합니다.

### 공정성 부족

공정성이란 "AI 시스템이 편견과 차별이 없고 모든 사람을 공정하고 평등하게 대우하도록 보장하는 것"으로 정의됩니다. 생성형 AI의 세계에서는 소외된 집단에 대한 배타적인 세계관이 모델의 결과물에 의해 강화되지 않아야 합니다.

이러한 유형의 결과물은 사용자에게 긍정적인 제품 경험을 제공하는 데 방해가 될 뿐만 아니라 사회적으로도 해를 끼칠 수 있습니다. 애플리케이션 개발자로서 우리는 생성형 AI로 솔루션을 구축할 때 항상 폭넓고 다양한 사용자층을 염두에 두어야 합니다.

## 생성형 AI를 책임감 있게 사용하는 방법

이제 책임감 있는 생성형 AI의 중요성을 확인했으니, 책임감 있게 AI 솔루션을 구축하기 위해 취할 수 있는 4가지 단계를 살펴보겠습니다:

![Mitigate Cycle](../../images/mitigate-cycle.png?WT.mc_id=academic-105485-koreyst)

### 잠재적 피해 측정

소프트웨어 테스트에서는 애플리케이션에 대한 사용자의 예상 행동을 테스트합니다. 마찬가지로 사용자가 많이 사용할 가능성이 높은 다양한 프롬프트 집합을 테스트하는 것은 잠재적인 피해를 측정하는 좋은 방법입니다.

우리 스타트업은 교육용 제품을 만들고 있으므로 교육 관련 프롬프트 목록을 준비하는 것이 좋습니다. 특정 주제, 역사적 사실, 학생 생활에 대한 프롬프트 등을 다룰 수 있습니다.

### 잠재적 피해 완화

이제 이 모델과 그 응답들로 인한 잠재적 피해를 예방하거나 제한할 수 있는 방법을 찾아야 할 때입니다. 이를 네 가지 측면에서 살펴볼 수 있습니다:

![Mitigation Layers](../../images/mitigation-layers.png?WT.mc_id=academic-105485-koreyst)

- **Model**. 올바른 사용 사례에 적합한 모델 선택. GPT-4와 같이 규모가 크고 복잡한 모델은 더 작고 구체적인 사용 사례에 적용할 경우 유해 콘텐츠의 위험이 더 커질 수 있습니다. 학습 데이터를 사용하여 미세 조정하면 유해한 콘텐츠의 위험도 줄일 수 있습니다.

- **Safety System**. 안전 시스템은 모델을 제공하는 플랫폼에서 피해를 완화하는 데 도움이 되는 일련의 도구 및 구성입니다. 예를 들어 Azure OpenAI 서비스의 콘텐츠 필터링 시스템을 들 수 있습니다. 시스템은 또한 탈옥 공격과 봇의 요청과 같은 원치 않는 활동을 감지해야 합니다.

- **Metaprompt**. 메타 프롬프트(Metaprompts)와 그라운딩(grounding)은 특정 행동과 정보를 기반으로 모델에게 지시하거나 제한할 수 있는 방법입니다. 시스템 입력을 사용하여 모델의 특정 한계를 정의할 수 있습니다. 또한 시스템의 범위 또는 도메인과 더 관련성이 높은 출력을 제공할 수도 있습니다. 또한 검색 증강 생성(Retrieval Augmented Generation, RAG)과 같은 기술을 사용하여 모델이 신뢰할 수 있는 일부 소스로부터만 정보를 가져오도록 할 수도 있습니다. 이 과정의 뒷부분에 [검색 애플리케이션 구축](../../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)에 대한 강의가 있습니다.

- **User Experience**. 마지막 레이어는 사용자가 애플리케이션의 인터페이스를 통해 모델과 어떤 방식으로든 직접 상호작용하는 곳입니다. 이러한 방식으로 사용자가 모델에 보낼 수 있는 입력 유형과 사용자에게 표시되는 텍스트 또는 이미지를 제한하도록 UI/UX를 설계할 수 있습니다. AI 애플리케이션을 배포할 때는 생성형 AI 애플리케이션이 할 수 있는 일과 할 수 없는 일에 대해서도 투명하게 알려야 합니다. [AI 애플리케이션을 위한 UX 디자인](../../../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)을 주제로 한 전체 강의가 준비되어 있습니다.

- **Evaluate model**. 모델이 학습된 데이터를 항상 제어할 수 있는 것은 아니기 때문에 LLM으로 작업하는 것은 어려울 수 있습니다. 그럼에도 불구하고 항상 모델의 성능과 결과물을 평가해야 합니다. 모델의 정확성, 유사성, 근거, 출력의 관련성을 측정하는 것은 여전히 중요합니다. 이는 이해관계자와 사용자에게 투명성과 신뢰를 제공하는 데 도움이 됩니다.

### 책임감 있는 생성형 AI 솔루션 운영

AI 애플리케이션을 중심으로 운영 사례를 구축하는 것이 마지막 단계입니다. 여기에는 모든 규제 정책을 준수하기 위해 법무 및 보안과 같은 스타트업의 다른 부서와 협력하는 것도 포함됩니다. 또한 출시 전에는 딜리버리, 예외상황 처리, 롤백(rollback)에 관한 계획을 수립하여 사용자 피해가 커지는 것을 방지하고자 합니다.

## 도구들

책임감 있는 AI 솔루션을 개발하는 작업이 복잡해 보일 수 있지만, 그만한 가치가 있는 작업입니다. 생성형 AI의 영역이 성장함에 따라 개발자가 책임감을 워크플로에 효율적으로 통합하는 데 도움이 되는 더 많은 도구들이 발전할 것입니다. 예를 들어 [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)는 API 요청을 통해 유해한 콘텐츠와 이미지를 탐지하는 데 도움을 줄 수 있습니다.

## Knowledge check

책임감 있는 AI 사용을 위해 주의해야 할 사항은 무엇인가요?

1. 답변이 정확해야 합니다.
1. AI가 범죄 목적으로 유해하게 사용되지 않도록 해야 합니다.
1. AI에 편견과 차별이 없는지 확인합니다.

답변: 2번과 3번이 맞습니다. 책임감 있는 AI는 유해한 영향과 편견을 완화하는 방법 등을 고려하는 데 도움이 됩니다.

## 🚀 Challenge

[Azure AI Content Saftey](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)에 대해 자세히 알아보고 사용 환경에 맞게 채택할 수 있는 사항을 확인하세요.

## 잘 하셨습니다. 학습을 계속하세요.

이 장을 완료한 후, [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)을 확인하여 제너레이티브 AI 지식의 수준을 계속 높여보세요!

다음 강의에서 [Prompt Engineering Fundamentals](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)를 살펴볼 것입니다!